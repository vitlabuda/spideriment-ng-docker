# Spideriment-NG â€“ Default Docker setup

This setup's [`docker-compose.yml`](docker-compose.yml) file consists of the following services:
- the **Spideriment-NG** web crawling daemon
- the **Spiderimeng-NG Search** search engine deployed using uWSGI
- the **nginx** web server which makes the uWSGI-deployed search engine available through HTTP
- the **Tor** daemon for anonymizing web traffic generated by the crawler
- the **MariaDB** database for storing crawled data & carrying out searches
- the **memcached** memory cache for caching `robots.txt` files downloaded by the crawler



## Deployment
Assuming you are in this repository's root directory:
```shell
cd 001_default/


# >>> Clone the repositories containing 'spideriment-ng' and 'spiderimeng-ng-search'
./clone_repos.sh


# >>> Configure 'spideriment-ng' and 'spiderimeng-ng-search'
# The 'spideriment-ng.docker.toml' and 'spideriment-ng-search.docker.toml' configuration files will be 
#  bind-mounted into the containers from the 'configs' directory by Docker.
cd configs/
cp spideriment-ng.docker.example.toml spideriment-ng.docker.toml
cp spideriment-ng-search.docker.example.toml spideriment-ng-search.docker.toml
nano spideriment-ng.docker.toml  # Use an editor you prefer...
nano spideriment-ng-search.docker.toml  # Use an editor you prefer...
cd ../


# >>> Build & run everything
# The web search engine will be accessible through HTTP on the Docker-published port 8031.
# NOTE: The crawler's termination process may take some time (it strongly depends on the load of the database server), 
#  so remember to set an appropriate timeout for container shutdowns (5 minutes should almost always be okay, 
#  setting 10 minutes here to be absolutely sure).
sudo docker compose up -t 600
```
